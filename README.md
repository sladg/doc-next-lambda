# Deploying Next to Lambda
Summary of knowledge in regards to deploying NextJS to Amazon Lambdas.

_TL;DR: NextJS in standalone mode + [lwa layer](https://github.com/awslabs/aws-lambda-web-adapter) = deployment on Function URL._

## Problems

### Network adapter

NextJS expects normal HTTP request and response objects whilst AWS Lambda together with other AWS services provide event object. This incompatibility results in problematic translation of incoming data and results.

Resources and solutions:
- https://www.npmjs.com/package/serverless-http
- https://github.com/aws-samples/lwa-nextjs-response-streaming-example
- https://aws.amazon.com/blogs/compute/using-response-streaming-with-aws-lambda-web-adapter-to-optimize-performance/
- https://github.com/awslabs/aws-lambda-web-adapter
- build custom HTTP server which translates some methods to compatible ones

- [ ] TBD describe solution

### Cold starts
Next server takes quite a bit of time to fire-up at the beggining. Vercel is working 

- [ ] TBD describe solution, 

### Caching
Next uses variety of caches and different mechanisms to improve performance. This results in rather over-complicated config for CloudFront.
Additional problem comes from Lambda's non-writable storage. Next will try to cache (write it) sometimes, this operation can fail, but results in lost performance.

EFS is complicate for setup and requires VPC, S3 is better option. Way to handle this globally is to patch FS functions with custom ones.
There are multiple places where Next uses memory and/or filesystem as place to save data. Occurences are:
- next/image uses it to save optimized image,
- ISR uses it to invalidate/cache data.

To ensure proper workings, we do following:
- `experimental.isrMemoryCacheSize` is set to `0` to turn-off in-memory cache for ISR,
- entrypoint JS file patches FS before initializing Next server (see: https://github.com/sladg/doc-next-lambda/blob/master/runner.js and https://github.com/sladg/doc-next-lambda/blob/master/s3fs.ts),
- we set `CACHE_BUCKET_NAME` env var to lambda pointing to read-write accessible bucket lambda can use as cache.


### Binaries
Lambda's runtime (if we ignore containerized option), uses Amazon Linux (multiple version options). This results in some of the binaries being possibly incompatible compared in runtime as buildtime used different OS / architecture. This is mostly notable in Prisma as their binaries take quite a lot of space.

- [ ] TBD describe solution
      Possible to solve with Docker Lambda. If we use Container type of deployment, we will install dependencies and build in target host so this problem is overcome.

### Size

Assets, chunks, bundles and other things generated by Next can take huge space. On a medium CRM-style projects, we can expect `node_modules` taking 100MB. `standalone` server taking around 5MB and static assets (typically located in `.next/static`) taking 25MB, additionally, `public` folder can contain rather large amount of assets as well. This means we cannot feasibly store statically generated assets inside Lambda as we would cross size limit rather quickly.

With this known, we need to route part of the traffic away from Next's server. To serve these files, we can use S3, however, we add complexity with new service type. Possible middle-ground would be using Docker container which has size up to 10G.

- [ ] TBD describe solution

## Knowledge
- One of the first topics on ISR in non-Vercel environment. Main outcome is Vercel providing guide with basics.
  <br/>
  https://github.com/vercel/next.js/discussions/19589

- Vercel's guide to self-hosting. Most limited to non-serverless and containerised applications. Useful information about `isrMemoryCacheSize`
  <br/>
  https://nextjs.org/docs/app/building-your-application/deploying#docker-image

- SAM template allowing Next on Lambda with streaming support. Primary infromation is env variable `AWS_LWA_INVOKE_MODE: response_stream` and `InvokeMode: RESPONSE_STREAM`.
  <br/>
  https://github.com/aws-samples/lwa-nextjs-response-streaming-example/blob/main/template.yaml

- Minimal example in Express showing how to use LWA outside of container, aka. in native runtime. This is implementable for NextJS.
  <br/>
  https://github.com/awslabs/aws-lambda-web-adapter/tree/main/examples/expressjs-zip

- OG pioneer of runing HTTP servers on Lambda.
  <br/>
  https://github.com/apparentorder/reweb

- Solution for deploying NextJS via CDK & SST. It prepares neat ZIPs to be uploaded to AWS as ready functions. Could be used with Terraform as well.
  <br/>
  https://github.com/sst/open-next

- CDK construct built on top of open-next. It's smaller and more straigh forward than SST.
  <br/>
  https://github.com/jetbridge/cdk-nextjs

- Benchmark of different coldstarts for different size of instance and code. TL;DR docker is on average faster than big native functions.
  <br/>
  https://mikhail.io/serverless/coldstarts/aws/

- Caching deep-dive for Next. Description of quite a few examples of what happens when.
  <br/>
  https://github.com/vercel/next.js/discussions/54075
  

## Testing and benchmarks

Next 13.5 takes 800-900ms to initialise in Lambda native Node environment (very similar result for Alpine container on Lambda). This happens once-per-instance, meaning, this instance can deal with multiple requests without re-starting. Increased load on application will result in spin-up of multiple new instances, each taking this time to start.

<img width="712" alt="image" src="https://github.com/sladg/doc-next-lambda/assets/26263265/cc2d494e-8bad-4679-a500-5690e411f454">

Next 14.0.3 takes 200-800ms to initialize in Lambda's container environment. This is big improvement compared to v13, however, this speed depends on size of the project.

